---
title: "Week 6: Data Anonymisation"
subtitle: ""
author: "<large>Charlotte Hadley</large>"
format: 
  revealjs:
    theme: "css/lecture-styles.scss"
    slide-number: "c/t"
bibliography: "../bibtex-file.bib"
csl: "../nature.csl"
cache: true
---

## Topics for today

```{r}
library(tidyverse)
library(gt)
library(readxl)
library(janitor)
```

1. Why is data anonymisation important?

1. What are specific risks of deanonymisation of health data?

1. Anonymity measures: k-anonymity and l-diversity

1. Case studies of deanonymisation

    > ... and why anonymity measures are often not enough

1. R packages for working with anonymouse data and sampling

-----------

## Why is data anonymisation important? {background-color="#23241F" .center .center-x}

-----------

## Data isn't always captured knowingly

Mostly during this course we've been talking about surveys or studies where data is explicitly being collected - and participants willingly submit their data.

<br>

But that's often not the case.

<br>

> Data is collected continuously about individuals without their explicit consent - and often without implicit consent.

-----------

## Data tracking across websites

XXX Tracking cookies. They bad. They identify individuals.

-----------

## Data tracking across credit cards

XXX Bad

-----------

## Data tracking during hospital visits

There's an A&E waiting time survey that's sent to folks that attend A&E.

This survey is sent without patients opting into it.

> The Section 251 of the NHS Act 2006 [@uk_government_national_2006] provides for the use of confidential patient data without consent for a specific purpose by the HRA or the Secretary of State for Health and Social Care.

This means that there are people looking at this data and deciding who to target.

Note that the NHS makes sure there's an opt out [once you've been invited]

> However, as has always been the case, patients/service users must be given the opportunity to opt-out.

-----------

> ## Data anonymisation is important because data is everywhere

We as individuals need to think about how our data is collected. how we use data in our day-to-day lives and in our careers.

XXX Whistleblowing is important

-----------

## Specific risks of deanonymisation of health data {background-color="#23241F" .center .center-x}

-----------

## Who can be at risk of deanonymisation?

:::: {.columns}

::: {.column width="48%"}

:::  {.subheading} 
Individuals 
:::

It's the dangers to individuals that we should primarily be concerned with.

<br>

There are significant risks to individual liberty, livelihood and life from deanonymisation.

:::

::: {.column width="4%"}

:::

::: {.column width="48%"}

:::  {.subheading} 
Organisations 
:::

However, organisations also suffer from deanonymisation.

<br>

- Organisations might suffer reputational damage

- Organisations might suffer legal difficulties

- XXX Fines? XXX

:::

:::: 

> Let's focus on the individual for now. The ICO provides a useful guide to managing data protection risk designed for organisations[@information_commissioners_office_anonymisation_2012]

-----------

## What is health data again?

Recall how the Data Protection Act[@uk_government_data_2018] identifies three types of health data:

- **“biometric data”** means personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of an individual, which allows or confirms the unique identification of that individual, such as facial images or dactyloscopic data;

- **“data concerning health”** means personal data relating to the physical or mental health of an individual, including the provision of health care services, which reveals information about his or her health status;

- **“genetic data”** means personal data relating to the inherited or acquired genetic characteristics of an individual which gives unique information about the physiology or the health of that individual and which results, in particular, from an analysis of a biological sample from the individual in question;

-----------

### Specific risks of deanonymised health data (I)

First and foremost, there is a risk of:

- Information about someone’s private life ending up in the public domain.

<br>

This in and of itself should be of concern, but more specifically

-	Individuals might suffer distress, embarrassment, or anxiety due to sensitive information being in the public domain

<br>

There is also a significant risk from sensitive information being in the public domain that:

- Individuals might suffer harassment, attack and/or injury 

- Individuals might suffer persecution

-----------

### Specific risks of deanonymised health data (II)

- Sensitive information might be sold to third-party organisations resulting in targeted advertising.

-----------

## XXX Case study XXX

Sending baby leaflets to a woman who ordered a pregnancy test?

Is that okay?!

-----------

### Specific risks of deanonymised health data (III)

- Sensitive information might be sold to third-party organisations results in a change in service options or costs

XXX Generally financial loss? XXX

XXX I need some evidence of this last one XXX

-----------

## Case studies of deanonymisation {background-color="#23241F" .center .center-x}

# Anonymity measures: k-anonymity and l-diversity {background-color="#23241F" .center .center-x}


-----------

## Early evidence for de-anonymisation (DRAFT)

In the late 90s there was a rapid conceptualisation of how easy de-anonymisation is of large, public datasets.

Sweeney showed in 1997[@sweeney_weaving_1997] that the Massachusetts voting list (n=54,805) could easily be de-anonymised

<img src='images/week-06/sweeney-mass-de-anonyisation.png'/>

-----------

## k-anonymity (DRAFT)

Good slides: https://courses.cs.duke.edu/fall13/compsci590.3/slides/lec4.pdf

-----------

## k-anonymity and high-dimensionality (DRAFT)

> "Furthermore, k-anonymization completely fails on high-dimensional datasets [2], such as the Netflix Prize dataset and most real-world datasets of individual recommendations and purchases." - Arvind Narayanan and Vitaly Shmatikov[@narayanan_robust_2008] 

They reference Aggarwal [@aggarwal_k-anonymity_2005] who showed through simulations the weakness of k-anonymity in high-dimensional datasets.

- The statistical modelling in the paper is fairly complex

- However, the experimental analysis section is fairly accessible and makes clear how weak k-anonymity is.

> When the data sets contained more than 45 dimensions, almost all the data points violated the 2-anonymity condition

-----------

## Modern/State-of-the-art Anonymisation (I)

There isn't a one size fits all[@rodriguez_current_2022] methodology for anonymising data.

<br>

On going research is identifying more complex[@zhang_efficient_2017] and robust[@palanisamy_privacy-preserving_2018] methodologies for both measuring and reducing the risk of de-anonymisation.

-----------

## Modern/State-of-the-art Anonymisation (II)

In fact, there are many "off-the-shelf" products which can be used to anonymise datasets.

:::: {.columns}

::: {.column width='40%'}
<img src='images/week-06/off-the-shelf-data-anonymisation.png'/>
:::

::: {.column width='60%'}
In 2021 Zuo et al[@zuo_data_2021] performed a systematic review of de-anonymisation tools in digital health care.

<br>

There were two off-the-shelf tools identified that were built with R

- [`{sdcMicro}`](http://sdctools.github.io/sdcMicro/index.html)

  - This package contains many tools for measuring/exploring the anonymity of datasets via a {shiny} app.
  
- [`{ShinyAnonymizer}](https://github.com/mariosggg/ShinyAnonymizer)

  - This package provides a {shiny} app for anonymising healthcare data

:::

::::

-----------

## R packages for working with anonymouse data and sampling {background-color="#23241F" .center .center-x}

# Case Study: Netflix De-anonymisation {background-color="#23241F" .center .center-x}

-----------

## Netlix Case Study: What happened?

Arvind Narayanan and Vitaly Shmatikov[@narayanan_robust_2008] 


# Case Study: Massachusettes	governor attack

# Case Study: AOL Privacy Breach

# Social Network attacks

-----------

XXX DRAGONS XXXX

## USEFUL RESOURCES:

- https://course.ece.cmu.edu/~ece734/fall2019/lectures/9-deanonymization.pdf - Database Privacy: k-anonymity and
de-anonymization attacks... gets nice and technical!


-----------

### References

::: {#refs}
:::

