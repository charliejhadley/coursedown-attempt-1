<!DOCTYPE html>
<html lang="en"><head>
<script src="week-07_slides_ethics-in-algorithms_files/libs/clipboard/clipboard.min.js"></script>
<script src="week-07_slides_ethics-in-algorithms_files/libs/quarto-html/tabby.min.js"></script>
<script src="week-07_slides_ethics-in-algorithms_files/libs/quarto-html/popper.min.js"></script>
<script src="week-07_slides_ethics-in-algorithms_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="week-07_slides_ethics-in-algorithms_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="week-07_slides_ethics-in-algorithms_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="week-07_slides_ethics-in-algorithms_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.0.37">

  <meta name="author" content="Charlotte Hadley">
  <title>Week 7: Ethics in Algorithms</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/twitter-widget-0.0.1/widgets.js"></script>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title">Week 7: Ethics in Algorithms</h1>
  <p class="author"><large>Charlotte Hadley</large></p>
</section>

<section>
<section id="ethics-in-algorithms-what-does-it-mean" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>Ethics in algorithms… what does it mean?</h1>

</section>
<section id="ethics-in-algorithms-what-does-it-mean-1" class="slide level2">
<h2>Ethics in algorithms… what does it mean?</h2>
<div class="cell" data-hash="week-07_slides_ethics-in-algorithms_cache/revealjs/unnamed-chunk-1_e2535c876a15d324d202aba5c5c4f2cb">

</div>
<div class="columns">
<div class="column" style="width:48%;">
<p>“Ethics in algorithms” paints a picture that we only need to consider ethics when writing algorithms.</p>
<p><br></p>
<p>Last week we discussed data anonymisation (and re-identification) and a lot of our conversation was about the ethics of <em>data collection</em>.</p>
<p><br></p>
<blockquote>
<p>Is it ethical for data about us to be collected without our knowledge?</p>
</blockquote>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<p>Data ethics is a <strong>might</strong> be a better term for what we’re discussing.</p>
<p><br></p>
<p>Data ethics includes, but isn’t limited to:</p>
<ul>
<li><p>How data is collected</p></li>
<li><p>How data is processed (by algorithms)</p></li>
<li><p>How data is consumed to build products and services</p></li>
<li><p>How data is released</p></li>
</ul>
<p><br></p>
<p>Companies are hiring data ethicists in droves.</p>
</div>
</div>
</section>
<section class="slide level2">

<h4 id="data-ethics-you-mean-machine-learning-big-data-and-ai">Data ethics… you mean machine learning, big data and AI!</h4>
<div class="columns">
<div class="column" style="width:48%;">
<p>Machine learning, big data and AI are all tremendously exciting and definitely always require <em>some</em> data<span class="citation" data-cites="sucholutsky_less_2021"><sup><a href="#/ref-sucholutsky_less_2021" role="doc-biblioref" onclick="return false;">1</a></sup></span>.</p>
<p><br></p>
<p>But data ethics is important every time we’re dealing with sensitive and/or private data.</p>
<p>Even if we’ve got survey data about attitudes to green spaces during the pandemic.</p>
<p><br></p>
<p>In this course when we talk about data ethics we’re also talking about the ethics of algorithms.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<div class="cell" data-hash="week-07_slides_ethics-in-algorithms_cache/revealjs/unnamed-chunk-2_7f1ca8d4ad43a577a1d8fff19e699fd9">
<div class="cell-output-display">
<blockquote class="twitter-tweet" data-width="550" data-lang="en" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">In Data Science, 80% of time spent prepare data, 20% of time spent complain about need for prepare data.</p>— Big Data Borat (@BigDataBorat) <a href="https://twitter.com/BigDataBorat/status/306596352991830016?ref_src=twsrc%5Etfw">February 27, 2013</a></blockquote>

</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="ethics-moral-philosophy" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>Ethics &amp; Moral Philosophy</h1>

</section>
<section id="ethics-moral-philosophy-1" class="slide level2">
<h2>Ethics &amp; Moral Philosophy</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>Moral philosophy and the history of ethics is fascinating.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<blockquote>
<p>We’re just going to completely ignore it.</p>
</blockquote>
</div>
</div>
</section></section>
<section>
<section id="right-to-privacy" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>Right to privacy</h1>

</section>
<section id="right-to-privacy-1" class="slide level2">
<h2>Right to privacy</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p>The Universal Declaration of Human Rights<span class="citation" data-cites="united_nations_general_assembly_universal_1948"><sup><a href="#/ref-united_nations_general_assembly_universal_1948" role="doc-biblioref" onclick="return false;">2</a></sup></span> provides universal right to privacy.</p>
<p><br></p>
<p>We’re going to extend this to include right to privacy in data collection and sharing of data.</p>
</div><div class="column" style="width:60%;">
<p><span style="color:cornflowerblue;font-weight:bold">Article 12</span></p>
<blockquote>
<p>No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence, nor to attacks upon his honour and reputation. Everyone has the right to the protection of the law against such interference or attacks.</p>
</blockquote>
</div>
</div>
<p><br></p>
<p><br></p>
<p><br></p>
<blockquote>
<p>But let’s look at algorithms a little bit more first.</p>
</blockquote>
</section></section>
<section>
<section id="self-driving-cars" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>Self-driving cars</h1>

</section>
<section id="self-driving-cars-i" class="slide level2">
<h2>Self-driving cars (I)</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>There’s a continuum of “self-driving car” techology. For discussion purposes, let’s define the tech as follows:</p>
<blockquote>
<p>A car on public roads under the control of an automated driving system that controls acceleration, breaking and driving direction. The driver does not have active control of the car, but can disengage the automated driving system and resume control.</p>
</blockquote>
<p>What ethical questions does the addition of self-driving cars to <strong>existing</strong> road networks raise?</p>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<ul>
<li><p>Who is responsible for a road traffic collision?</p></li>
<li><p>How does the automated driving system respond to trolley problem situations?</p>
<ul>
<li><p>Does the car prioritise reducing risk to the driver or a pedestrian</p></li>
<li><p>Does the car prioritise reducing risk to other vehicles or pedestrians?</p></li>
</ul></li>
</ul>
</div>
</div>
</section>
<section id="self-driving-cars-ii" class="slide level2">
<h2>Self-driving cars (II)</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p>Awad et al<span class="citation" data-cites="awad_moral_2018"><sup><a href="#/ref-awad_moral_2018" role="doc-biblioref" onclick="return false;">3</a></sup></span> surveyed 2.3million people in 2018 to explore attitudes to moral dilemmas faced by autonomous vehicles.</p>
<p><br></p>
<p>Attitudes vary by geographic region of participants and the demographics of potential <em>accident victims</em>.</p>
<p><br></p>
<blockquote>
<p>Should the ethical decision making of autonomous vehicles vary dependent on location?!</p>
</blockquote>
</div><div class="column" style="width:2%;">

</div><div class="column" style="width:58%;">
<p><img src="images/week-07/self-driving-cars_moral-compass.png"></p>
</div>
</div>
<div class="footnote">
<p>This data visualisation is derived from Awad et al<span class="citation" data-cites="awad_moral_2018"><sup><a href="#/ref-awad_moral_2018" role="doc-biblioref" onclick="return false;">3</a></sup></span> but appears in Maxmen 2018<span class="citation" data-cites="maxmen_self-driving_2018"><sup><a href="#/ref-maxmen_self-driving_2018" role="doc-biblioref" onclick="return false;">4</a></sup></span></p>
</div>
</section>
<section id="self-driving-cars-iii" class="slide level2">
<h2>Self-driving cars (III)</h2>
<div class="columns">
<div class="column" style="width:58%;">
<p>Reliable and unbiased estimations of autonomous vehicle safety are difficult to find - and interpret.</p>
<p><br></p>
<p>As more autonomous vehicles are used by untrained drivers in real-world circumstances we’ll collect more information about their safety.</p>
<p><br></p>
<p>But it’s important we try and understand <em>how</em> autonomous vehicles make their decisions.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:38%;">
<p>This statistic is repeated <strong>everywhere</strong> even in 2022</p>
<blockquote>
<p>Self-driving cars also have an accident rate of 9.1 crashes per million miles driven – which is more than double that of regular vehicles.</p>
</blockquote>
<p>It’s derived from a 2015 report<span class="citation" data-cites="schoettle_preliminary_2015"><sup><a href="#/ref-schoettle_preliminary_2015" role="doc-biblioref" onclick="return false;">5</a></sup></span>… and a total of 11 collisions.</p>
</div>
</div>
</section>
<section id="self-driving-cars-iv" class="slide level2">
<h2>Self-driving cars (IV)</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>Self-driving vehicles use <strong>many</strong> different tools for sensing and measuring their environment.</p>
<p><br></p>
<p>There’s considerable debate [and patent dispute] about two competing technologies: LiDAR and Computer Vision.</p>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<ul>
<li>Computer vision uses “regular cameras” and categorises what it sees.</li>
</ul>
<p><img src="images/week-07/machine-vision.gif"></p>
<ul>
<li>LiDAR continuously pulses UV light, measures reflection times and builds a 3D visual map. This data is then clustered and classified to identify things around the car.</li>
</ul>
</div>
</div>
</section>
<section id="self-driving-cars-iv-1" class="slide level2">
<h2>Self-driving cars (IV)</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>As we question the ethical considerations of autonomous vehicles it’s crucial to understand <strong>how</strong> vehicles make decisions - particularly during accidents.</p>
<p><br></p>
<p>Does the vehicle make Fair, Accountable and Transparent (FAT) decisions?</p>
<div class="fragment" data-fragment-index="1">
<p><br></p>
<p>Thankfully, both LiDAR and computer vision provide surprisingly high <strong>transparency</strong>.</p>
<p><br></p>
<p>This table comes from a fatal collision report<span class="citation" data-cites="national_transport_safety_board_collision_2018"><sup><a href="#/ref-national_transport_safety_board_collision_2018" role="doc-biblioref" onclick="return false;">6</a></sup></span> between an autonomous vehicle and a pedestrian.</p>
<p><br></p>
<p>We can see the autonomous vehicle repeatedly, and inconsistently misidentified a pedestrian.</p>
</div>
</div><div class="column" style="width:2%;">

</div><div class="column fragment" data-fragment-index="1" style="width:48%;">
<p><img src="images/week-07/self-driving-car_collision-table.png"></p>
</div>
</div>
</section></section>
<section>
<section id="fairness-accountability-and-transparency" class="title-slide slide level1 center">
<h1>Fairness, Accountability and Transparency</h1>

</section>
<section id="fairness-accountability-and-transparency-1" class="slide level2">
<h2>Fairness, Accountability and Transparency</h2>
<div class="columns">
<div class="column fragment fade-in-then-semi-out" style="width:48%;">
<p>Since 2013<span class="citation" data-cites="hardt_occupy_2013"><sup><a href="#/ref-hardt_occupy_2013" role="doc-biblioref" onclick="return false;">7</a></sup></span> we’ve used “Fairness, Accountability and Transparency” (FAT) to discuss the ethical development and application of algorithms.</p>
<p><br></p>
<p>The <a href="https://www.fatml.org/">FAT/ML conference series</a> ran from 2014 to 2018 and is a tremendously useful resource for examples of FAT concepts for your assessment.</p>
<p><br></p>
<p>The definitions for these terms are slightly loose. When explaining them please use your own words and examples.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<div class="fragment">
<blockquote>
<p><strong>Fairness</strong>. Does an algorithm have bias or does it discriminate against individuals or a specific group?</p>
<p>This is usually a consequence of bias in the underlying dataset.</p>
</blockquote>
</div>
<div class="fragment">
<blockquote>
<p><strong>Accountability</strong> is concerned with the responsibility for results of decisions made that are powered or otherwise influenced by an algorithm</p>
</blockquote>
</div>
<div class="fragment">
<blockquote>
<p><strong>Transparency</strong> is concerned with how algorithms fit into a decision making process.</p>
</blockquote>
</div>
</div>
</div>
</section></section>
<section>
<section id="fairness" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>Fairness</h1>

</section>
<section id="fairness-5-or-6-sources-of-bias" class="slide level2">
<h2>Fairness: 5 (or 6) Sources of Bias</h2>
<div class="columns">
<div class="column" style="width:48%;">
<blockquote>
<p>Frustratingly, I can’t find the source for this! But this list has been around since at least 2014<span class="citation" data-cites="barocas_big_2016"><sup><a href="#/ref-barocas_big_2016" role="doc-biblioref" onclick="return false;">8</a></sup></span>.</p>
</blockquote>
<p>Broadly in the literature (and in essays on the subject) there are 5 common sources of bias in training datasets.</p>
<hr>
<p>These biases result in unfair treatment of individuals/groups by an algorithm. Examples of unfair treatment include</p>
<ul>
<li><p>Refusal of service</p></li>
<li><p>More expensive services</p></li>
<li><p>Reduced range of services</p></li>
</ul>
<p>When these services include healthcare the ramifications can be life threatening.</p>
<div class="fragment" data-fragment-index="1">

</div>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<ul>
<li><p>Proxies</p></li>
<li><p>Limited features</p></li>
<li><p>Skewed sample</p></li>
<li><p>Tainted examples</p></li>
<li><p>Sample size disparities</p></li>
</ul>
<hr>
<div class="fragment" data-fragment-index="1">
<p>The assessment limits itself to these 5 sources.</p>
<p>It’s worthwhile mentioning that sometimes these bias is knowingly, and deliberately left (or added) to a training dataset.</p>
<p>This is called <strong>masking</strong><span class="citation" data-cites="barocas_big_2016"><sup><a href="#/ref-barocas_big_2016" role="doc-biblioref" onclick="return false;">8</a></sup></span>.</p>
</div>
</div>
</div>
<div class="footnote">
<p>I’m relying quite a bit on <a href="https://blog.dataiku.com/explaining-bias-in-your-data">Alexandre’s article for dataiku.com</a><span class="citation" data-cites="landeau_explaining_2020"><sup><a href="#/ref-landeau_explaining_2020" role="doc-biblioref" onclick="return false;">9</a></sup></span> and a <a href="https://towardsdatascience.com/sources-of-unintended-bias-in-training-data-be5b7f3347d0">Medium post from Cristina Goldfain</a><span class="citation" data-cites="goldfain_sources_2020"><sup><a href="#/ref-goldfain_sources_2020" role="doc-biblioref" onclick="return false;">10</a></sup></span>.</p>
</div>
</section>
<section id="fairness-proxies" class="slide level2">
<h2>Fairness: Proxies</h2>
<blockquote>
<p>Proxies are the easiest to identify and describe of these sources of bias.</p>
</blockquote>
<p>When training our algorithms we want to remove potentially biasing attributes like race, gender, age or religious belief - because we don’t want an algorithm to discriminate, disadvantage or advantage a specific group.</p>
<p><br></p>
<p>However, there are <strong>many</strong> other variables in a dataset that are <strong>proxies</strong> for these attributes. Can you think of any?</p>
<div class="fragment">
<ul>
<li>Systemic racism means that income, neighborhood and similiar variables are strong proxies for race.</li>
</ul>
<blockquote>
<p>“Redlining is the practice of arbitrarily denying or limiting financial services to specific neighborhoods, generally because its residents are people of color or are poor.”</p>
</blockquote>
<ul>
<li>Facebook friendships can be a strong proxy for sexual orientation<span class="citation" data-cites="jernigan_gaydar_2009"><sup><a href="#/ref-jernigan_gaydar_2009" role="doc-biblioref" onclick="return false;">11</a></sup></span></li>
</ul>
</div>
</section>
<section id="fairness-limited-features" class="slide level2">
<h2>Fairness: Limited Features</h2>
<blockquote>
<p>This is a harder bias to describe (and to verify). It’s also <a href="https://twitter.com/charliejhadley/status/1574814439815221251?s=20&amp;t=81SYBivy0JoYEglUyWWW-A"><strong>extremely frustrating</strong> to pin down examples</a>.</p>
</blockquote>
<p><br></p>
<p>Limited features is a consequence of having smaller datasets for minority groups, or specific combinations of sensitive attributes.</p>
<p><br></p>
<p>Individuals in these groups will be subject to less accurate classifications (or predictions) than other groups.</p>
<p><br></p>
<p>This article gives a theoretical example: https://towardsdatascience.com/sources-of-unintended-bias-in-training-data-be5b7f3347d0</p>
<p><br></p>
<blockquote>
<p>Limited features is a pre-requisite for skewed samples, our next topic.</p>
</blockquote>
</section>
<section id="fairness-skewed-sample-i" class="slide level2">
<h2>Fairness: Skewed sample (I)</h2>
<blockquote>
<p>It’s kind of unfair this is collected together with proxies, limited features, tainted examples, sample size disparities</p>
</blockquote>
<p><br></p>
<p>Skewed samples is a bias in your algorithm when existing skewness due to limited features results in an algorithm that <strong>becomes more skewed over time</strong> - creating a biased feedback loop<span class="citation" data-cites="ensign_runaway_2017"><sup><a href="#/ref-ensign_runaway_2017" role="doc-biblioref" onclick="return false;">12</a></sup></span>. Please do read the Ensign et al’s<span class="citation" data-cites="ensign_runaway_2017"><sup><a href="#/ref-ensign_runaway_2017" role="doc-biblioref" onclick="return false;">12</a></sup></span> paper from 2017 as it explains this really well.</p>
<blockquote>
<p>Given historical crime incident data for a collection of regions, decide how to allocate patrol officers to areas to detect crime</p>
<p>…</p>
<p>Since such discovered incidents only occur in neighborhoods that police have been sent to by the predictive policing algorithm itself, there is the potential for this sampling bias to be compounded, causing a runaway feedback loop.</p>
</blockquote>
</section>
<section id="fairness-skewed-sample-google-flu-i" class="slide level2">
<h2>Fairness: Skewed sample Google Flu (I)</h2>
<blockquote>
<p>Another really great example with an easy to follow about the skewed samples behind issues with Google Flu’s predictive power, <a href="https://www.davidlazer.com/sites/default/files/The%20Parable%20of%20Google%20Flu_FinalFinal.pdf">LAzer et al 2014</a><span class="citation" data-cites="lazer_parable_2014"><sup><a href="#/ref-lazer_parable_2014" role="doc-biblioref" onclick="return false;">13</a></sup></span></p>
</blockquote>
<p>In 2009 Google published details about their Google Flu Trends (GFT) prediction engine<span class="citation" data-cites="ginsberg_detecting_2009"><sup><a href="#/ref-ginsberg_detecting_2009" role="doc-biblioref" onclick="return false;">14</a></sup></span> that used search results to predict flu outbreaks.</p>
<p><br></p>
<p>The theory being that ill people search for symptoms before they would be otherwise detected.</p>
<p><br></p>
<p>However, the whole thing is somewhat questionable as</p>
<blockquote>
<p>GFT has never documented the 45 search terms used, and the examples that have been released appear misleading<span class="citation" data-cites="cook_assessing_2011"><sup><a href="#/ref-cook_assessing_2011" role="doc-biblioref" onclick="return false;">15</a></sup></span></p>
<p>Source: <a href="https://www.davidlazer.com/sites/default/files/The%20Parable%20of%20Google%20Flu_FinalFinal.pdf">LAzer et al 2014</a><span class="citation" data-cites="lazer_parable_2014"><sup><a href="#/ref-lazer_parable_2014" role="doc-biblioref" onclick="return false;">13</a></sup></span></p>
</blockquote>
</section>
<section id="fairness-skewed-sample-google-flu-ii" class="slide level2">
<h2>Fairness: Skewed sample Google Flu (II)</h2>
<div class="columns">
<div class="column" style="width:56%%;">
<p>Coincidentally, the first big failure of the service was the 2009 A–H1N1 outbreak<span class="citation" data-cites="cook_assessing_2011"><sup><a href="#/ref-cook_assessing_2011" role="doc-biblioref" onclick="return false;">15</a></sup></span>.</p>
<p><br></p>
<p>The GFT didn’t detect the non-seasonal wave of H1N1 during the summer of 2009.</p>
<p><br></p>
<p>Google quickly updated the GFT algorithm later in 2009 and the improved algorithm did a better job of predicting the outbreak.</p>
</div><div class="column" style="width:1%;">

</div><div class="column" style="width:45%;">
<p><img src="images/week-07/skewed_google-flu-2009.png"></p>
</div>
</div>
</section>
<section id="fairness-skewed-sample-google-flu-iii" class="slide level2">
<h2>Fairness: Skewed sample Google Flu (III)</h2>
<div class="columns">
<div class="column" style="width:56%%;">
<p>However, from 2011 onwards the GFT consistently overestimated flu outbreaks<span class="citation" data-cites="lazer_parable_2014"><sup><a href="#/ref-lazer_parable_2014" role="doc-biblioref" onclick="return false;">13</a></sup></span> and became increasingly unreliable.</p>
<p>Lazer et al<span class="citation" data-cites="lazer_parable_2014"><sup><a href="#/ref-lazer_parable_2014" role="doc-biblioref" onclick="return false;">13</a></sup></span> posit two reasons for this:</p>
<ul>
<li>Big data hubris</li>
</ul>
<p>Was the algorithm a glorified winter detector?!</p>
</div><div class="column" style="width:1%;">

</div><div class="column" style="width:45%;">
<p><img src="images/week-07/skewed-samples_google-flu.png"> Source: Lazer et al<span class="citation" data-cites="lazer_parable_2014"><sup><a href="#/ref-lazer_parable_2014" role="doc-biblioref" onclick="return false;">13</a></sup></span></p>
</div>
</div>
<ul>
<li>Algorithm dynamic</li>
</ul>
<p>Google has many competing algorithms at play. The service has a recommendation engine that shows potentially interesting (or useful?) search terms. Developments in the recommendation algorithm can negatively effect the GFT as “flu search terms” could be shown to healthy people.</p>
</section>
<section id="fairness-tainted-examples-i" class="slide level2">
<h2>Fairness: Tainted examples (I)</h2>
<div class="columns">
<div class="column" style="width:48%;">
<div class="fragment fade-in-then-semi-out">
<p>We need to talk about supervised learning.</p>
<p><br></p>
<p>Do you know the difference between supervised and unsupervised learning?</p>
<p><br></p>
</div>
<div class="fragment fade-in-then-semi-out">
<p>In supervised learning we create categories and assign training data to these categories.</p>
<p>The algorithm will classify all future inputs as belonging to one of these categories.</p>
</div>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<div class="fragment fade-in-then-semi-out">
<p>That’s all well and good if we’re doing something like classifying the <a href="https://www.wolfram.com/mathematica/new-in-10/highly-automated-machine-learning/predict-the-survival-of-titanic-passengers.html">titanic survival dataset</a>.</p>
<p>… but often our categories aren’t absolute, eg</p>
<ul>
<li><p>Google user has the flu!</p></li>
<li><p>Rating a driver’s likelihood of an accident</p></li>
<li><p>Rating a “good job candidate”</p></li>
</ul>
<blockquote>
<p>Let’s look into the job candidate example more</p>
</blockquote>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="fairness-tainted-examples-in-recruitment-i">Fairness: Tainted examples in Recruitment (I)</h3>
<div class="columns">
<div class="column" style="width:48%;">
<p>Algorithms are used ubiquitously and sometimes aggressively in HR and recruitment. Fraij and László published a literature review on the topic in 2021<span class="citation" data-cites="fraij_literature_2021"><sup><a href="#/ref-fraij_literature_2021" role="doc-biblioref" onclick="return false;">16</a></sup></span>.</p>
<p><br></p>
<p>These algorithms are often baked into software to automatically filter through CVs.</p>
<p><br></p>
<p>The algorithms are celebrated for being fast, efficient and allowing recruiters to focus on more interesting and human required tasks.</p>
<blockquote>
<p>This couldn’t possibly create issues</p>
</blockquote>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<p>This is an example of potential tainted example bias because there isn’t a universal truth to who is a good job candidate.</p>
<p><br></p>
<p>There are at least two different methods for categorising “good” candidates:</p>
<ul>
<li><p>Who gets hired. And we know there are <a href="https://www.theguardian.com/world/2019/jan/17/minority-ethnic-britons-face-shocking-job-discrimination">“shocking levels of discrimination in hiring”</a><span class="citation" data-cites="siddique_minority_2019"><sup><a href="#/ref-siddique_minority_2019" role="doc-biblioref" onclick="return false;">17</a></sup></span></p></li>
<li><p>Work evaluation scores of recruited people. There’s strong evidence of discrimination in these scores<span class="citation" data-cites="stauffer_existence_2005"><sup><a href="#/ref-stauffer_existence_2005" role="doc-biblioref" onclick="return false;">18</a></sup></span>.</p></li>
</ul>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="fairness-sample-size-disparity-i">Fairness: Sample Size Disparity (I)</h3>
<blockquote>
<p>Unfortunately, even perfectly balanced [untainted and unskewed] training data with untainted categories can have bias. Sample size disparity is a common example of this.</p>
</blockquote>
<div class="columns">
<div class="column" style="width:48%;">
<p>Sample size disparity exists when completely balanced datasets have disparity in the size of subgroups.</p>
<p><br></p>
<p>This disparity results in unfair behaviour of the algorithm when applied to real-world datasets.</p>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<p>The most widely cited (and easy to follow) example of this is the Nymwars of 2011 - <a href="https://en.wikipedia.org/wiki/Nymwars">https://en.wikipedia.org/wiki/Nymwars</a>.</p>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="fairness-sample-size-disparity-ii">Fairness: Sample Size Disparity (II)</h3>
<div class="columns">
<div class="column" style="width:48%;">
<p>In 2011 Google implemented a real name policy for Google+.</p>
<p><br></p>
<p>There are significant privacy issues with this policy.</p>
<p><br></p>
<p>But users also found their names rejected as “not real”.</p>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<p>Folks with names outside of the first and last name pattern - eg Charlie Hadley - found their names rejected.</p>
<p><br></p>
<p>This is widely blamed on the training datasets having sample size disparity - there were fewer instances of other name forms.</p>
<p><br></p>
<p>I recommend reading this article <a href="https://shinesolutions.com/2018/01/08/falsehoods-programmers-believe-about-names-with-examples/">Falsehoods Programmers Believe About Names – With Examples</a><span class="citation" data-cites="rogers_falsehoods_2018"><sup><a href="#/ref-rogers_falsehoods_2018" role="doc-biblioref" onclick="return false;">19</a></sup></span>.</p>
</div>
</div>
</section></section>
<section>
<section id="some-more-case-studies" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>… some more case studies</h1>

</section>
<section class="slide level2">

<h4 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-i">“Man is to computer programmer as woman is to homemaker” (I)</h4>
<div class="columns">
<div class="column" style="width:48%;">
<p>In 2016 Bolukbasi et al<span class="citation" data-cites="bolukbasi_man_2016"><sup><a href="#/ref-bolukbasi_man_2016" role="doc-biblioref" onclick="return false;">20</a></sup></span> demonstrated how the extremely common <strong>word2vec</strong> algorithm shows significant gender-bias.</p>
<p><br></p>
<p><strong>word2vec</strong> is a patented algorithm<span class="citation" data-cites="mikolov_computing_2015"><sup><a href="#/ref-mikolov_computing_2015" role="doc-biblioref" onclick="return false;">21</a></sup></span> that - when trained on a given text corpus - will find similar/synonymous words for a given input.</p>
<p><a href="https://code.google.com/archive/p/word2vec/">Google provides a useful introduction</a> to how the system works.</p>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<blockquote>
<p>For example, if you enter ‘france’, distance will display the most similar words and their distances to ‘france’, which should look like:</p>
</blockquote>
<div class="cell" data-hash="week-07_slides_ethics-in-algorithms_cache/revealjs/unnamed-chunk-3_af06cb104daa47d7837c37da06f72fb6">
<div class="cell-output-display">

<div id="xqhrpmdxhr" style="overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

#xqhrpmdxhr .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#xqhrpmdxhr .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#xqhrpmdxhr .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#xqhrpmdxhr .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#xqhrpmdxhr .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xqhrpmdxhr .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#xqhrpmdxhr .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#xqhrpmdxhr .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#xqhrpmdxhr .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#xqhrpmdxhr .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#xqhrpmdxhr .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#xqhrpmdxhr .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#xqhrpmdxhr .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#xqhrpmdxhr .gt_from_md > :first-child {
  margin-top: 0;
}

#xqhrpmdxhr .gt_from_md > :last-child {
  margin-bottom: 0;
}

#xqhrpmdxhr .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#xqhrpmdxhr .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#xqhrpmdxhr .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#xqhrpmdxhr .gt_row_group_first td {
  border-top-width: 2px;
}

#xqhrpmdxhr .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#xqhrpmdxhr .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#xqhrpmdxhr .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#xqhrpmdxhr .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xqhrpmdxhr .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#xqhrpmdxhr .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#xqhrpmdxhr .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#xqhrpmdxhr .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#xqhrpmdxhr .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#xqhrpmdxhr .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#xqhrpmdxhr .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#xqhrpmdxhr .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#xqhrpmdxhr .gt_left {
  text-align: left;
}

#xqhrpmdxhr .gt_center {
  text-align: center;
}

#xqhrpmdxhr .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#xqhrpmdxhr .gt_font_normal {
  font-weight: normal;
}

#xqhrpmdxhr .gt_font_bold {
  font-weight: bold;
}

#xqhrpmdxhr .gt_font_italic {
  font-style: italic;
}

#xqhrpmdxhr .gt_super {
  font-size: 65%;
}

#xqhrpmdxhr .gt_two_val_uncert {
  display: inline-block;
  line-height: 1em;
  text-align: right;
  font-size: 60%;
  vertical-align: -0.25em;
  margin-left: 0.1em;
}

#xqhrpmdxhr .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

#xqhrpmdxhr .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#xqhrpmdxhr .gt_slash_mark {
  font-size: 0.7em;
  line-height: 0.7em;
  vertical-align: 0.15em;
}

#xqhrpmdxhr .gt_fraction_numerator {
  font-size: 0.6em;
  line-height: 0.6em;
  vertical-align: 0.45em;
}

#xqhrpmdxhr .gt_fraction_denominator {
  font-size: 0.6em;
  line-height: 0.6em;
  vertical-align: -0.05em;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1">word</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1">cosine.distance</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td class="gt_row gt_left">spain</td>
<td class="gt_row gt_right">0.678515</td></tr>
    <tr><td class="gt_row gt_left">belgium</td>
<td class="gt_row gt_right">0.665923</td></tr>
    <tr><td class="gt_row gt_left">netherlands</td>
<td class="gt_row gt_right">0.652428</td></tr>
    <tr><td class="gt_row gt_left">italy</td>
<td class="gt_row gt_right">0.633130</td></tr>
    <tr><td class="gt_row gt_left">switzerland</td>
<td class="gt_row gt_right">0.622323</td></tr>
    <tr><td class="gt_row gt_left">luxembourg</td>
<td class="gt_row gt_right">0.610033</td></tr>
    <tr><td class="gt_row gt_left">portugal</td>
<td class="gt_row gt_right">0.577154</td></tr>
    <tr><td class="gt_row gt_left">russia</td>
<td class="gt_row gt_right">0.571507</td></tr>
    <tr><td class="gt_row gt_left">germany</td>
<td class="gt_row gt_right">0.563291</td></tr>
    <tr><td class="gt_row gt_left">catalonia</td>
<td class="gt_row gt_right">0.534176</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
</div>
</div>
</div>
</section>
<section class="slide level2">

<h4 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-ii">“Man is to computer programmer as woman is to homemaker” (II)</h4>
<div class="columns">
<div class="column" style="width:38%;">
<p><strong>word2vec</strong> is pre-trained on ~100 billion words from Google News articles.</p>
<p><br></p>
<p>Some users of the alogirthm will use the pre-trained corpus, others will use their own.</p>
<p><br></p>
<p>Bolukbasi et al<span class="citation" data-cites="bolukbasi_man_2016"><sup><a href="#/ref-bolukbasi_man_2016" role="doc-biblioref" onclick="return false;">20</a></sup></span> investigated how the algorithm behaved with the pre-trained corpus, with the assumption:</p>
<blockquote>
<p>[the data would] exhibit little gender bias because many of its authors are professional journalists</p>
</blockquote>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:58%;">
<p><img src="images/week-07/word-2-vec_gender-extremes.png"></p>
</div>
</div>
</section>
<section class="slide level2">

<h4 id="man-is-to-computer-programmer-as-woman-is-to-homemaker-iii">“Man is to computer programmer as woman is to homemaker” (III)</h4>
<div class="columns">
<div class="column" style="width:48%;">
<p>What sources of bias are included in this example?</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<ul>
<li><p>Proxies</p></li>
<li><p>Limited features</p></li>
<li><p>Skewed sample</p></li>
<li><p>Tainted examples</p></li>
<li><p>Sample size disparities</p></li>
</ul>
</div>
</div>
</section>
<section id="racial-bias-in-photography-i" class="slide level2">
<h2>Racial bias in photography (I)</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>Smartphone manufacturers have recently started advertising how their devices <a href="https://store.google.com/intl/en/discover/realtone/">accurately photograph the beauty of all skin tones</a><span class="citation" data-cites="google_real_2022"><sup><a href="#/ref-google_real_2022" role="doc-biblioref" onclick="return false;">22</a></sup></span>.</p>
<p><br></p>
<p>This has been a <a href="https://www.youtube.com/watch?v=d16LNHIEJzs">known issue in photography for decades</a>, in many different situations.</p>
<div class="fragment" data-fragment-index="1">
<p><br></p>
<p>In pre-digital photography days, Kodak issued “Shirley cards” for colour-balancing images. These cards exclusively contained Caucasian models until the 1990s.</p>
</div>
</div><div class="column" style="width:4%;">

</div><div class="column center-x fragment" data-fragment-index="1" style="width:48%;">
<p><img src="images/week-07/shirley-card_polaroid.png"></p>
<p>Source: Lorna Roth 2009<span class="citation" data-cites="roth_looking_2009-1"><sup><a href="#/ref-roth_looking_2009-1" role="doc-biblioref" onclick="return false;">23</a></sup></span></p>
</div>
</div>
</section>
<section id="racial-bias-in-photography-ii" class="slide level2">
<h2>Racial bias in photography (II)</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>This issue has also been pervasive in digital photography.</p>
<p><br></p>
<p>The continued use of the Fitzpatrick skin tone scale is largely to blame.</p>
<p><br></p>
<p>Dr.&nbsp;Ellis Monk’s research<span class="citation" data-cites="monk_unceasing_2021"><sup><a href="#/ref-monk_unceasing_2021" role="doc-biblioref" onclick="return false;">24</a></sup></span> has culminated in the Monk Skin Tone Scale with <a href="https://skintone.google/the-scale">evidence improvements in processing of images compared to the Fitzpatrick scale</a>.</p>
</div><div class="column" style="width:4%;">

</div><div class="column center-x" style="width:48%;">
<p><img src="images/week-07/fitzpatrick-skin-types.png"></p>
<p>Source: Ward et al<span class="citation" data-cites="ward_table_2017"><sup><a href="#/ref-ward_table_2017" role="doc-biblioref" onclick="return false;">25</a></sup></span></p>
</div>
</div>
</section>
<section id="racial-bias-in-photography-iii" class="slide level2">
<h2>Racial bias in photography (III)</h2>
<p>I’d like to recommend some additional resources to learn about this:</p>
<ul>
<li><p>Vox Media’s video essay <a href="https://www.youtube.com/watch?v=d16LNHIEJzs">Color film was built for white people. Here’s what it did to dark skin.</a><span class="citation" data-cites="vox_color_2015"><sup><a href="#/ref-vox_color_2015" role="doc-biblioref" onclick="return false;">26</a></sup></span>.</p></li>
<li><p><a href="https://www.nytimes.com/2019/04/25/lens/sarah-lewis-racial-bias-photography.html">The Racial Bias Built Into Photography by the NY Times</a><span class="citation" data-cites="lewis_racial_2019"><sup><a href="#/ref-lewis_racial_2019" role="doc-biblioref" onclick="return false;">27</a></sup></span></p></li>
<li><p>Lorna Roth’s research paper <a href="https://cjc.utpjournals.press/doi/full/10.22230/cjc.2009v34n1a2196">Looking at Shirley, the Ultimate Norm: Colour Balance, Image Technologies, and Cognitive Equity</a><span class="citation" data-cites="roth_looking_2009-1"><sup><a href="#/ref-roth_looking_2009-1" role="doc-biblioref" onclick="return false;">23</a></sup></span></p></li>
<li><p>Google’s <a href="https://skintone.google/">Skin Tone Research</a> with Dr.&nbsp;Ellis Monk.</p></li>
</ul>
</section>
<section id="racial-bias-in-face-analysis-i" class="slide level2">
<h2>Racial bias in face analysis (I)</h2>
<div class="columns">
<div class="column" style="width:40%;">
<p>This is a fundamental problem in all face analysis and face detection software.</p>
<p><br></p>
<p><a href="https://twitter.com/jovialjoy?lang=en">Joy Buolamwini</a> does a much better job of explaining this in 5 minutes than I can.</p>
</div><div class="column center-x" style="width:60%;">
<p><a href="https://youtu.be/TWWsW1w-BVo"><img src="images/week-07/joy-buolamwini.png"></a></p>
<p>Source: <a href="https://gendershades.org">gendershades.org</a><span class="citation" data-cites="mit_media_lab_gender_2018"><sup><a href="#/ref-mit_media_lab_gender_2018" role="doc-biblioref" onclick="return false;">28</a></sup></span></p>
</div>
</div>
</section>
<section id="racial-bias-in-face-analysis-photography" class="slide level2">
<h2>Racial bias in face analysis &amp; photography</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>What sources of bias are included in this example?</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<ul>
<li><p>Proxies</p></li>
<li><p>Limited features</p></li>
<li><p>Skewed sample</p></li>
<li><p>Tainted examples</p></li>
<li><p>Sample size disparities</p></li>
</ul>
</div>
</div>
</section></section>
<section>
<section id="accountability-transparency" class="title-slide slide level1 center">
<h1>Accountability &amp; Transparency</h1>

</section>
<section id="accountability-transparency-1" class="slide level2">
<h2>Accountability &amp; Transparency</h2>
<p>These two terms are used interchangeably and confusingly. That’s okay. In the assessment discuss how these two concepts relate to one another and you’ll be fine.</p>
<p><br></p>
<p>There’s a really useful definition given by the <em>EU “governance framework for algorithmic accountability and transparency”</em><span class="citation" data-cites="european_parliament_directorate_general_for_parliamentary_research_services_governance_2019"><sup><a href="#/ref-european_parliament_directorate_general_for_parliamentary_research_services_governance_2019" role="doc-biblioref" onclick="return false;">29</a></sup></span></p>
<blockquote>
<p>The primary role of transparency is identified as a tool to enable accountability. If it is not known what an organisation is doing, it cannot be held accountable and cannot be regulated.</p>
<p>…</p>
<p>An important difference between transparency and accountability is that accountability is primarily a legal and ethical obligation on an individual or organisation to account for its activities, accept responsibility for them, and to disclose the results in a transparent manner.</p>
</blockquote>
<p>… but the rest of the document is very heavy reading.</p>
</section>
<section id="accountability" class="slide level2">
<h2>Accountability</h2>
<p>For a more thorough exploration of accountability I recommend reading Caplan et al<span class="citation" data-cites="caplan_algorithmic_2018"><sup><a href="#/ref-caplan_algorithmic_2018" role="doc-biblioref" onclick="return false;">30</a></sup></span>.</p>
</section>
<section id="transparency-and-gaming-the-system" class="slide level2">
<h2>Transparency and gaming the system</h2>
<p>It’s worthwhile mentioning that transparency can result in “gaming the system” in which an algorithm is applied:</p>
<blockquote>
<p>Also, in some cases, transparency may lead to groups and individuals “gaming the system.” For example, even the minimal openness surrounding how the trending feature on Twitter surfaces topics has allowed it to be manipulated into covering certain topics by bots and coordinated groups of individuals. Therefore, different contexts may call for different levels of transparency.</p>
<p>Source: Caplan et al 2018<span class="citation" data-cites="caplan_algorithmic_2018"><sup><a href="#/ref-caplan_algorithmic_2018" role="doc-biblioref" onclick="return false;">30</a></sup></span></p>
</blockquote>
<ul>
<li>How else could we game algorithms?</li>
</ul>
</section></section>
<section>
<section id="other-sources-of-unfairness" class="title-slide slide level1 center">
<h1>Other sources of unfairness</h1>

</section>
<section id="other-sources-of-unfairness-1" class="slide level2">
<h2>Other sources of unfairness</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>Earlier we tried to categorise sources of bias:</p>
<p>-Proxies</p>
<p>-Limited features</p>
<p>-Skewed sample</p>
<p>-Tainted examples</p>
<p>-Sample size disparities</p>
<p>These are almost always concerned with the training data behind algorithms.</p>
<p><br></p>
<p>There’s a whole universe of ways we can bias an algorithm by <em>how it’s applied</em>.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<p>Let’s look at some of these!</p>
</div>
</div>
</section>
<section class="slide level2">

<h3 id="fairness-and-abstraction-in-sociotechnical-systems">Fairness and Abstraction in Sociotechnical Systems</h3>
<div class="columns">
<div class="column" style="width:48%;">
<p>Selbst et al published an excellent paper in 2019<span class="citation" data-cites="selbst_fairness_2019"><sup><a href="#/ref-selbst_fairness_2019" role="doc-biblioref" onclick="return false;">31</a></sup></span> on “abstraction traps”.</p>
<p><br></p>
<p>These traps are designed to help us account for the interactions between the technical systems behind our algorithms and the social world in which they’re applied.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<p>I think they’re really useful and give us further context for our discussion of fairness.</p>
<p>The next several slides explores these traps.</p>
</div>
</div>
</section>
<section id="selbst-abstraction-the-framing-trap" class="slide level2">
<h2>Selbst Abstraction: The Framing Trap</h2>
<blockquote>
<p>Failure to model the entire system over which a social criterion, such as fairness, will be enforced</p>
</blockquote>
<p>Example:</p>
<p>In the US criminal justice pipeline algorithmically trained risk assessment tools are used to predict the “risk” of a defendant and determine of parole is awarded.</p>
<p>&lt;br</p>
<p>However - usually this is the risk the defendant fails to appear at court hearings. Reoffending is only occassionally considered in these models.</p>
<p><br></p>
<p>These risk assessment tools are presented only as recommendations to judges. They do not account for consistently (see<span class="citation" data-cites="selbst_fairness_2019"><sup><a href="#/ref-selbst_fairness_2019" role="doc-biblioref" onclick="return false;">31</a></sup></span>) different consideration between judges.</p>
<blockquote>
<p>The intended appliction of the algorithmally trained risk assessment does <strong>not</strong> take into account their use by judges and therefore the apparent fairness cannot be measured</p>
</blockquote>
</section>
<section id="selbst-abstraction-the-portability-trap" class="slide level2">
<h2>Selbst Abstraction: The Portability Trap</h2>
<blockquote>
<p>Failure to understand how repurposing algorithmic solutions designed for one social context may be misleading, inaccurate, or otherwise do harm when applied to a different context</p>
</blockquote>
<p>Here’s a really nice quote</p>
<blockquote>
<p>Within computer science, it is considered good practice to design a system that can be used for different tasks in different contexts. “But what that does is ignore a lot of social context,” says Selbst. “You can’t have a system designed in Utah and then applied in Kentucky directly because different communities have different versions of fairness. Or you can’t have a system that you apply for ‘fair’ criminal justice results then applied to employment. How we think about fairness in those contexts is just totally different.”</p>
<p>Source: Karen Hao<span class="citation" data-cites="hao_this_2019"><sup><a href="#/ref-hao_this_2019" role="doc-biblioref" onclick="return false;">32</a></sup></span></p>
</blockquote>
</section>
<section id="selbst-abstraction-the-formalism-trap" class="slide level2">
<h2>Selbst Abstraction: The Formalism Trap</h2>
<blockquote>
<p>Failure to account for the full meaning of social concepts such as fairness, which can be procedural, contextual, and contestable, and cannot be resolved through mathematical formalisms</p>
</blockquote>
<div class="columns">
<div class="column" style="width:48%;">
<p>I did promise we would ignore moral philosophy, but it’s important for a moment.</p>
<blockquote>
<p>Fairness and discrimination are complex concepts that philosophers, sociologists, and lawyers have long debated. They are at times procedural, contextual, and politically contestable, and each of those properties is a core part of the concepts themselves<span class="citation" data-cites="selbst_fairness_2019"><sup><a href="#/ref-selbst_fairness_2019" role="doc-biblioref" onclick="return false;">31</a></sup></span></p>
</blockquote>
</div><div class="column" style="width:4%;">

</div><div class="column fragment" style="width:48%;">
<p>In algorithmic recruitment practices we don’t particularly mind about false positives (candidates who aren’t a good fit) because there’s an interview to filter them out.</p>
<p><br></p>
<p>However, the justicial risk assessments mentioned earlier might require the opposite treatment. False positives will result in parole being refused.</p>
</div>
</div>
</section>
<section id="selbst-abstraction-the-ripple-effect-trap" class="slide level2">
<h2>Selbst Abstraction: The Ripple Effect Trap</h2>
<blockquote>
<p>Failure to understand how the insertion of technology into an existing social system changes the behaviors and embedded values of the pre-existing system</p>
</blockquote>
<p>Selbst et al<span class="citation" data-cites="selbst_fairness_2019"><sup><a href="#/ref-selbst_fairness_2019" role="doc-biblioref" onclick="return false;">31</a></sup></span> depend entirely on the justicial risk assessments example for this. Their increased use focuses the intent of the justice review system to incapacitation.</p>
</section>
<section id="selbst-abstraction-the-solutionism-trap" class="slide level2">
<h2>Selbst Abstraction: The Solutionism Trap</h2>
<blockquote>
<p>Failure to recognize the possibility that the best solution to a problem may not involve technology</p>
</blockquote>
<blockquote>
<p>Modeling requires pinning down definitions. Code calcifies. When fairness is a politically contested, movable issue, a model may not be able to capture the facets of how it moves</p>
</blockquote>
</section></section>
<section>
<section id="compas-algorithmic-risk-assessments" class="title-slide slide level1 center">
<h1>COMPAS: Algorithmic risk assessments</h1>

</section>
<section id="compas-algorithmic-risk-assessments-1" class="slide level2">
<h2>COMPAS: Algorithmic risk assessments</h2>
<div class="columns">
<div class="column" style="width:48%;">
<p>The algorithmic risk assessment algorithm we’ve discussed is called COMPAS.</p>
<p><br></p>
<p>It’s been subject to <strong>intensive</strong> study.</p>
<p><br></p>
<p>ProPublica’s study in 2016<span class="citation" data-cites="mattu_machine_2016"><sup><a href="#/ref-mattu_machine_2016" role="doc-biblioref" onclick="return false;">33</a></sup></span> was one of the first investigations. A technical breakdown of this is provided by the same authors<span class="citation" data-cites="mattu_how_2016"><sup><a href="#/ref-mattu_how_2016" role="doc-biblioref" onclick="return false;">34</a></sup></span>.</p>
</div><div class="column" style="width:4%;">

</div><div class="column" style="width:48%;">
<center>
<img src="images/week-07/COMPAS-accuracy.png" height="300px">
</center>
<p>The algorithm performs worse than 400 random Mechanical Turk users<span class="citation" data-cites="dressel_accuracy_2018"><sup><a href="#/ref-dressel_accuracy_2018" role="doc-biblioref" onclick="return false;">35</a></sup></span>.</p>
<p>I’d like to show you <a href="https://www.youtube.com/watch?v=p-82YeUPQh0">Hany Farid’s TED tak on this topic</a><span class="citation" data-cites="tedx_talks_danger_2018"><sup><a href="#/ref-tedx_talks_danger_2018" role="doc-biblioref" onclick="return false;">36</a></sup></span>.</p>
</div>
</div>
</section></section>
<section>
<section id="inequality-vs-justice" class="title-slide slide level1 center">
<h1>Inequality vs Justice</h1>

</section>
<section id="inequality-vs-justice-1" class="slide level2">
<h2>Inequality vs Justice</h2>
<p>I thought it was important to not skip over the continuum beyond inequality and justice. These cartoons are from <a href="https://twitter.com/johnmaeda">John Maeda’s</a> presentation on <a href="https://design.co/design-in-tech-report-2019-no-track/#1">Design in Tech Report 2019</a><span class="citation" data-cites="maeda_presentation_2019"><sup><a href="#/ref-maeda_presentation_2019" role="doc-biblioref" onclick="return false;">37</a></sup></span> and designed by <a href="https://twitter.com/lunchbreath">Tony Ruth</a>.</p>

<img src="images/week-07/inequality-through-justice.png" class="r-stretch"></section></section>
<section>
<section id="we-have-to-stop-somewhere" class="title-slide slide level1 center center-x" data-background-color="#23241F">
<h1>We have to stop somewhere</h1>

</section>
<section id="we-have-to-stop-somewhere-1" class="slide level2">
<h2>We have to stop somewhere</h2>
<p>We’ve covered more than enough for your assessment and to get a feel for the complexities of ethics in algorithms.</p>
<p>We didn’t really discuss data privacy very much. I’d recommend reading about the OECD Fair Information Practices.</p>
</section>
<section class="slide level2 smaller scrollable">

<h3 id="references">References</h3>
<div id="refs" class="references csl-bib-body" data-line-spacing="2" role="doc-bibliography">
<div id="ref-sucholutsky_less_2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline">Sucholutsky, I. &amp; Schonlau, M. <a href="https://doi.org/10.1609/aaai.v35i11.17171"><span>“<span>Less Than One</span>”</span>-<span>Shot Learning</span>: <span>Learning N Classes From M</span> <span><span class="math inline">\(&lt;\)</span></span> <span>N Samples</span></a>. <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> <strong>35</strong>, 9739–9746 (2021).</div>
</div>
<div id="ref-united_nations_general_assembly_universal_1948" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline">United Nations General Assembly. Universal <span>Declaration</span> of <span>Human Rights</span>. (1948).</div>
</div>
<div id="ref-awad_moral_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline">Awad, E. <em>et al.</em> <a href="https://doi.org/10.1038/s41586-018-0637-6">The <span>Moral Machine</span> experiment</a>. <em>Nature</em> <strong>563</strong>, 59–64 (2018).</div>
</div>
<div id="ref-maxmen_self-driving_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline">Maxmen, A. <a href="https://doi.org/10.1038/d41586-018-07135-0">Self-driving car dilemmas reveal that moral choices are not universal</a>. <em>Nature</em> <strong>562</strong>, 469–470 (2018).</div>
</div>
<div id="ref-schoettle_preliminary_2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline">Schoettle, B. &amp; Sivak, M. A <span>Preliminary Analysis</span> of <span>Real-World Crashes Involving Self-Driving Vehicles</span>. (2015).</div>
</div>
<div id="ref-national_transport_safety_board_collision_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline">National Transport Safety Board. Collision <span>Between Vehicle Controlled</span> by <span>Developmental Automated Driving System</span> and <span>Pedestrian</span>, <span>Tempe</span>, <span>Arizona</span>, <span>March</span> 18, 2018. (2018).</div>
</div>
<div id="ref-hardt_occupy_2013" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline">Hardt, M. Occupy <span>Algorithms</span>: <span>Will Algorithms Serve</span> the 99%? (2013).</div>
</div>
<div id="ref-barocas_big_2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Barocas, S. &amp; Selbst, A. D. Big <span>Data</span>’s <span>Disparate Impact</span>. (2016) doi:<a href="https://doi.org/10.2139/ssrn.2477899">10.2139/ssrn.2477899</a>.</div>
</div>
<div id="ref-landeau_explaining_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Landeau, A. Explaining <span>Bias</span> in <span>Your Data</span>. (2020).</div>
</div>
<div id="ref-goldfain_sources_2020" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Goldfain, C. Sources of unintended bias in training data. <em>Medium</em> (2020).</div>
</div>
<div id="ref-jernigan_gaydar_2009" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Jernigan, C. &amp; Mistree, B. F. T. Gaydar: <span>Facebook</span> friendships expose sexual orientation. <em>First Monday</em> (2009) doi:<a href="https://doi.org/10.5210/fm.v14i10.2611">10.5210/fm.v14i10.2611</a>.</div>
</div>
<div id="ref-ensign_runaway_2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline">Ensign, D., Friedler, S. A., Neville, S., Scheidegger, C. &amp; Venkatasubramanian, S. Runaway <span>Feedback Loops</span> in <span>Predictive Policing</span>. (2017) doi:<a href="https://doi.org/10.48550/arXiv.1706.09847">10.48550/arXiv.1706.09847</a>.</div>
</div>
<div id="ref-lazer_parable_2014" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline">Lazer, D., Kennedy, R., King, G. &amp; Vespignani, A. <a href="https://doi.org/10.1126/science.1248506">The <span>Parable</span> of <span>Google Flu</span>: <span>Traps</span> in <span>Big Data Analysis</span></a>. <em>Science</em> <strong>343</strong>, 1203–1205 (2014).</div>
</div>
<div id="ref-ginsberg_detecting_2009" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline">Ginsberg, J. <em>et al.</em> <a href="https://doi.org/10.1038/nature07634">Detecting influenza epidemics using search engine query data</a>. <em>Nature</em> <strong>457</strong>, 1012–1014 (2009).</div>
</div>
<div id="ref-cook_assessing_2011" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline">Cook, S., Conrad, C., Fowlkes, A. L. &amp; Mohebbi, M. H. <a href="https://doi.org/10.1371/journal.pone.0023610">Assessing <span>Google Flu Trends Performance</span> in the <span>United States</span> during the 2009 <span>Influenza Virus A</span> (<span>H1N1</span>) <span>Pandemic</span></a>. <em>PLOS ONE</em> <strong>6</strong>, e23610 (2011).</div>
</div>
<div id="ref-fraij_literature_2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline">FraiJ, J. &amp; László, V. <a href="https://doi.org/10.21791/IJEMS.2021.1.10">A literature <span>Review</span>: <span>Artificial Intelligence Impact</span> on the <span>Recruitment Process</span></a>. <em>International Journal of Engineering and Management Sciences</em> <strong>6</strong>, 108–119 (2021).</div>
</div>
<div id="ref-siddique_minority_2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline">Siddique, H. Minority ethnic <span>Britons</span> face ’shocking’ job discrimination. <em>The Guardian</em> (2019).</div>
</div>
<div id="ref-stauffer_existence_2005" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline">Stauffer, J. M. &amp; Buckley, M. R. <a href="https://doi.org/10.1037/0021-9010.90.3.586">The <span>Existence</span> and <span>Nature</span> of <span>Racial Bias</span> in <span>Supervisory Ratings</span></a>. <em>Journal of Applied Psychology</em> <strong>90</strong>, 586–591 (2005).</div>
</div>
<div id="ref-rogers_falsehoods_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline">rogers, tony. Falsehoods <span>Programmers Believe About Names</span> - <span>With Examples</span>. <em>Shine Solutions Group</em> (2018).</div>
</div>
<div id="ref-bolukbasi_man_2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline">Bolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V. &amp; Kalai, A. Man is to <span>Computer Programmer</span> as <span>Woman</span> is to <span>Homemaker</span>? <span>Debiasing Word Embeddings</span>. (2016) doi:<a href="https://doi.org/10.48550/arXiv.1607.06520">10.48550/arXiv.1607.06520</a>.</div>
</div>
<div id="ref-mikolov_computing_2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline">Mikolov, T., Chen, K., Corrado, G. S. &amp; Dean, J. A. Computing numeric representations of words in a high-dimensional space. (2015).</div>
</div>
<div id="ref-google_real_2022" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline">Google. Real <span>Tone</span> on <span>Google Pixel</span>. <em>Google Store</em> (2022).</div>
</div>
<div id="ref-roth_looking_2009-1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline">Roth, L. <a href="https://doi.org/10.22230/cjc.2009v34n1a2196">Looking at <span>Shirley</span>, the <span>Ultimate Norm</span>: <span>Colour Balance</span>, <span>Image Technologies</span>, and <span>Cognitive Equity</span></a>. <em>Canadian Journal of Communication</em> <strong>34</strong>, 111–136 (2009).</div>
</div>
<div id="ref-monk_unceasing_2021" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline">Monk, E. P., Jr. <a href="https://doi.org/10.1162/daed_a_01847">The <span>Unceasing Significance</span> of <span>Colorism</span>: <span>Skin Tone Stratification</span> in the <span>United States</span></a>. <em>Daedalus</em> <strong>150</strong>, 76–90 (2021).</div>
</div>
<div id="ref-ward_table_2017" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline">Ward, W. H., Lambreton, F., Goel, N., Yu, J. Q. &amp; Farma, J. M. <span>TABLE</span> 1, <span>Fitzpatrick Classification</span> of <span>Skin Types I</span> through <span>VI</span>. (2017).</div>
</div>
<div id="ref-vox_color_2015" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline">Vox. Color film was built for white people. <span>Here</span>’s what it did to dark skin. (2015).</div>
</div>
<div id="ref-lewis_racial_2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline">Lewis, S. The <span>Racial Bias Built Into Photography</span>. <em>The New York Times</em> (2019).</div>
</div>
<div id="ref-mit_media_lab_gender_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline">MIT Media Lab. Gender <span>Shades</span>. (2018).</div>
</div>
<div id="ref-european_parliament_directorate_general_for_parliamentary_research_services_governance_2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline">European Parliament. Directorate General for Parliamentary Research Services. <em>A governance framework for algorithmic accountability and transparency.</em> (<span>Publications Office</span>, 2019).</div>
</div>
<div id="ref-caplan_algorithmic_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline">Caplan, R., Donovan, J., Hanson, L. &amp; Matthews, J. Algorithmic <span>Accountability</span>: <span>A</span> primer. (2018).</div>
</div>
<div id="ref-selbst_fairness_2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline">Selbst, A. D., Boyd, D., Friedler, S. A., Venkatasubramanian, S. &amp; Vertesi, J. Fairness and <span>Abstraction</span> in <span>Sociotechnical Systems</span>. in <em>Proceedings of the <span>Conference</span> on <span>Fairness</span>, <span>Accountability</span>, and <span>Transparency</span></em> 59–68 (<span>Association for Computing Machinery</span>, 2019). doi:<a href="https://doi.org/10.1145/3287560.3287598">10.1145/3287560.3287598</a>.</div>
</div>
<div id="ref-hao_this_2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline">Hao, K. This is how <span>AI</span> bias really happensand why it’s so hard to fix. <em>MIT Technology Review</em> (2019).</div>
</div>
<div id="ref-mattu_machine_2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline">Mattu, L. K., Jeff Larson. Machine <span>Bias</span>. <em>ProPublica</em> (2016).</div>
</div>
<div id="ref-mattu_how_2016" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline">Mattu, L. K., Julia Angwin. How <span>We Analyzed</span> the <span>COMPAS Recidivism Algorithm</span>. <em>ProPublica</em> (2016).</div>
</div>
<div id="ref-dressel_accuracy_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline">Dressel, J. &amp; Farid, H. <a href="https://doi.org/10.1126/sciadv.aao5580">The accuracy, fairness, and limits of predicting recidivism</a>. <em>Science Advances</em> <strong>4</strong>, eaao5580 (2018).</div>
</div>
<div id="ref-tedx_talks_danger_2018" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline">TEDx Talks. The danger of predictive algorithms in criminal justice | <span>Hany Farid</span> | <span>TEDxAmoskeagMillyard</span>. (2018).</div>
</div>
<div id="ref-maeda_presentation_2019" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline">Maeda, J. Presentation: <span>Design</span> in <span>Tech Report</span> 2019. <em>Design in Tech</em> (2019).</div>
</div>
</div>
<div class="footer footer-default">

</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="week-07_slides_ethics-in-algorithms_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        function fireSlideChanged(previousSlide, currentSlide) {

          // dispatch for htmlwidgets
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for reveal
        if (window.Reveal) {
          window.Reveal.addEventListener("slidechanged", function(event) {
            fireSlideChanged(event.previousSlide, event.currentSlide);
          });
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          let href = ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>